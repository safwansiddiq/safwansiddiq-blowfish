
[{"content":"","date":"14 November 2025","externalUrl":null,"permalink":"/posts/","section":"Blog","summary":"","title":"Blog","type":"posts"},{"content":" The flow I had in mind Here is How I Went From Failed Attempts to a Working AI Workflow # I watch a lot of YouTube. Tutorials on automation, podcasts with founders, educational deep-dives on topics I\u0026rsquo;m curious about. The problem? I\u0026rsquo;d watch something brilliant, think \u0026ldquo;I need to remember this,\u0026rdquo; and then\u0026hellip; forget it all two days later.\nI needed a second brain for YouTube ‚Äî something that could automatically capture the insights, organize them with timestamps, and save them to my Notion workspace where I could actually reference them later. So I set out to build exactly that: a Telegram bot that turns any YouTube link into comprehensive, timestamped notes.\nSpoiler: It took way more attempts than I expected. Here\u0026rsquo;s the full story of how I went from complete failures to a workflow that actually works.\nThe Vision: What I Wanted to Build # The concept was simple:\nSend a YouTube link to a Telegram bot\nGet back detailed notes with clickable timestamps, key takeaways, and curated resources\nHave everything automatically saved to a Notion page\nThe tools I chose:\nn8n for workflow automation (self-hosted on Hostinger)\nOpenRouter for access to Claude 4.5 Sonnet and other AI models\nRapidAPI for fetching YouTube transcripts and metadata\nTelegram as my interface\nNotion as my knowledge repository\nSimple enough, right? Well, not exactly.\nPart 1: The First (Failed) Attempts at Getting a Transcript # My first thought was the \u0026ldquo;easy\u0026rdquo; approach: just scrape the transcript from the web. I\u0026rsquo;d used the HTTP Request node in n8n before, so I figured I could point it at a transcript website like youtubetranscripts.com, feed it the YouTube URL, and extract the text.\nThe reality? I got back this monstrosity:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;YouTube Transcript\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { font-family: Arial, sans-serif; } .transcript { ... } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; ... Pages and pages of HTML, CSS, JavaScript‚Äîeverything except a clean transcript. I tried different nodes, different websites, different approaches. Nothing worked. I was drowning in markup.\nüí° Lesson #1: Web scraping consumer-facing websites is brittle and unreliable. The structure can change at any moment, and you\u0026rsquo;re fighting against page layouts designed for humans, not machines. I needed something more robust.\nPart 2: The \u0026ldquo;Perplexity\u0026rdquo; Pivot - Letting AI Do the Heavy Lifting # Then I had what I thought was a brilliant idea: what if I just let the AI find the transcript for me? Tools like Perplexity can browse the web and extract information. I had access to web search tools like Tavily and Brave Search through n8n. Why not give my AI Agent one of those tools and let it handle the messy work?\nI installed the Tavily node, connected it to my AI Agent, and updated my prompt:\nWhen given a YouTube video link, use web search to find and retrieve the video\u0026#39;s transcript, then analyze it... I sent a test YouTube URL through the workflow. The error:\n{ \u0026#34;errorMessage\u0026#34;: \u0026#34;Bad request - please check your parameters\u0026#34;, \u0026#34;errorDescription\u0026#34;: \u0026#34;Query cannot consist only of site: operators. Please provide search terms.\u0026#34; } The problem? The AI was trying to pass the raw YouTube URL directly to Tavily as a search query. But Tavily expected actual search terms, not a URL. I needed to either pre-process the URL into a proper search query or find a completely different approach.\nI tried switching to different models (since Perplexity Sonar didn\u0026rsquo;t support tool use), tweaked the prompt, adjusted how Tavily was configured. Nothing worked consistently.\nüí° Lesson #2: AI tools aren\u0026rsquo;t magic. You can\u0026rsquo;t just give an AI a hammer and expect it to build a house. You need to understand how the tool works and structure your inputs appropriately. The AI is powerful, but it still needs guidance.\nPart 3: The API Breakthrough - Finding the Right Tool # Frustrated with all the workarounds, I went back to basics: just use an API specifically built for this task.\nA quick search on RapidAPI led me to the YouTube Transcripts API. Finally‚Äîa dedicated endpoint that returns clean, structured JSON with the transcript text, timestamps, and duration.\nI set up the HTTP Request node with the proper cURL parameters:\nGET https://youtube-transcripts.p.rapidapi.com/youtube/transcript ?url={{ videoUrl }} \u0026amp;videoId={{ videoId }} \u0026amp;chunkSize=100 \u0026amp;text=false \u0026amp;lang=en Headers:\nx-rapidapi-host: youtube-transcripts.p.rapidapi.com x-rapidapi-key: YOUR_API_KEY And for the first time\u0026hellip; it worked. I got back beautiful, clean JSON:\n{ \u0026#34;lang\u0026#34;: \u0026#34;en\u0026#34;, \u0026#34;content\u0026#34;: [ { \u0026#34;text\u0026#34;: \u0026#34;These are the five steps I always use when building AI agents...\u0026#34;, \u0026#34;offset\u0026#34;: 80, \u0026#34;duration\u0026#34;: 23840 }, ... ] } With this reliable data source, I could finally build the core workflow:\nTelegram Trigger ‚Üí Receives YouTube link\nIF Node ‚Üí Validates it contains \u0026ldquo;youtu\u0026rdquo;\nCode Node ‚Üí Extracts video ID from URL\nHTTP Request ‚Üí Calls RapidAPI for transcript\nCode Node ‚Üí Formats transcript with timestamps\nAI Agent ‚Üí Analyzes and creates notes\nNotion Node ‚Üí Saves the result\nTelegram Reply ‚Üí Sends confirmation with Notion link\nüí° Lesson #3: Use APIs whenever possible. They provide structured, predictable data. They\u0026rsquo;re maintained by people who care about uptime. They\u0026rsquo;re the foundation of robust automation.\nPart 4: From Generic Summaries to Detailed Notes # Now that I had the transcript, I needed the AI to actually do something useful with it. My first attempt was a simple prompt:\nAnalyze this YouTube transcript and create a summary. The result? A generic, high-level overview. Something like:\n\u0026ldquo;This video discusses five steps for building AI agents. The speaker covers foundations, identifying opportunities, process mapping, workflows vs agents, and guardrails.\u0026rdquo;\nTechnically accurate, but completely useless. I didn\u0026rsquo;t need a book report‚ÄîI needed notes. The kind of notes I\u0026rsquo;d take if I watched the video myself.\nThe Great Prompt Engineering Journey # This is where I spent most of my time. Here\u0026rsquo;s what I learned through iteration:\nChange #1: Set the Role and Perspective\nInstead of \u0026ldquo;summarize this,\u0026rdquo; I changed the opening to:\nYou are an expert note-taker who creates comprehensive, detailed notes from video transcripts. Your goal is to capture the substance, nuance, and creator\u0026#39;s voice as if someone watched the entire video and wrote everything down. Better. The AI started writing more like a human taking notes.\nChange #2: Content Type Detection\nI realized that a tutorial needs different formatting than a podcast. So I added:\nDetect the Content Type: - Tutorial/How-to: Focus on step-by-step instructions and implementation - Podcast/Interview: Extract stories, insights, and quotes - Educational: Capture concepts, explanations, and frameworks This made the AI adapt its structure to the type of content.\nChange #3: Clickable Timestamps\nThis was a game-changer. I wanted timestamps that I could click to jump to that exact moment in the video. The format:\n[2:30](https://www.youtube.com/watch?v=VIDEO_ID\u0026amp;t=150s) I added to the prompt:\nUse clickable timestamps frequently. Format: [MM:SS](videoURL\u0026amp;t=XXXs) where you convert MM:SS to seconds. Example: [2:30](videoURL\u0026amp;t=150s) The Timestamp Hallucination Problem\nThen I hit a bizarre issue: the AI was making up timestamps. It would reference [2:29] for a quote that actually appeared at [15:30]. The timestamps looked plausible, but they were completely wrong.\nThe problem? The AI was trying to be helpful and was generating timestamps instead of using the actual ones from the transcript.\nThe fix required a heavy-handed addition to the prompt:\n**CRITICAL TIMESTAMP RULES:** - ONLY use timestamps that appear in the transcript above - NEVER make up, estimate, or fabricate timestamps - When referencing content, find where it actually appears in the transcript - If you\u0026#39;re unsure, look back at the transcript to find the real timestamp This finally forced the AI to stick to reality.\nChange #4: Curated Resources via Tavily\nSince I still had the Tavily search tool connected, I added:\n### Curated Resources for Further Learning Using web search, find and recommend 5-7 high-quality resources related to the main topics covered in this video. Now the AI would search for related articles, books, and tools mentioned in the video and include them in the notes. üí° Lesson #4: Your prompt is your program. The difference between mediocre AI output and genuinely useful output comes down to how specific, structured, and constraint-heavy your prompt is. Treat it like code.\nPart 5: The Metadata Layer # The notes were good, but something was missing: context. Who made this video? What was it called? How long was it?\nI added a second RapidAPI call to fetch video metadata (title, channel, thumbnail, duration). Then I updated my prompt to start the notes with:\n![Thumbnail](thumbnailURL) **\u0026#34;Video Title\u0026#34;** by Channel Name This gave every note page a visual header, making it easier to recognize at a glance in Notion.\nThe Duration Conversion Problem The metadata returned duration in HH:MM:SS format (like 1:23:45), but I needed it in total seconds for the AI to reference. So I wrote a quick Code node:\nconst duration = $json.duration; const parts = duration.split(\u0026#39;:\u0026#39;).map(part =\u0026gt; parseInt(part, 10)); let seconds = 0; if (parts.length === 3) { seconds = (parts[0] * 3600) + (parts[1] * 60) + parts[2]; } else if (parts.length === 2) { seconds = (parts[0] * 60) + parts[1]; } else { seconds = parts[0]; } return { ...$json, lengthSeconds: seconds }; Now the AI had access to the duration in a usable format.\nPart 6: Error Handling - Making It Production-Ready # What happens if a video has captions disabled? Or if RapidAPI goes down for maintenance? Initially, the workflow would just\u0026hellip; fail silently.\nI would just wait and wait and wait\u0026hellip; and the results never comes.\nI needed error handling to avoid this.\nThe Setup:\nOn both RapidAPI HTTP Request nodes, I enabled \u0026ldquo;Continue on Fail\u0026rdquo;\nAfter each API call, I added an IF node to check for errors\nIf an error was detected, the workflow would:\nSend me a Telegram message explaining the issue\nStop execution to avoid cascading failures\nThe error message looked like:\nüòû **Captions Not Available** Video: [YouTube URL] ‚ùå This video doesn\u0026#39;t have public captions available or they\u0026#39;re disabled by the creator. üí° Lesson #5: A good workflow is a resilient workflow. Error handling, logging, and clear user feedback are what separate a personal hack from a tool you can actually rely on.\nPart 7: Choosing the Right AI Model # I experimented with several models on OpenRouter:\nGPT-4o (fast but sometimes shallow)\nGemini 2.5 Pro (great for long videos, very affordable)\nClaude 4.5 Sonnet (best overall for nuanced note-taking)\nFor this use case, Claude 4.5 Sonnet won. Here\u0026rsquo;s why:\n200K context window - Handles even 2+ hour video transcripts Superior reasoning - Captures nuance and preserves the creator\u0026rsquo;s voice Excellent tool use - Works seamlessly with the Tavily search integration Natural writing style - The notes don\u0026rsquo;t sound robotic Cost per video averaged around $0.08‚Äì$0.25 depending on length, which felt reasonable for the value I was getting.\nPart 8: The Final Workflow # Here\u0026rsquo;s what the completed automation looks like: This is the full N8N workflow Telegram Trigger ‚Üì IF: Contains \u0026#34;youtu\u0026#34;? ‚Üì Code: Extract Video ID ‚Üì HTTP Request: Get Transcript (RapidAPI) ‚Üì IF: Error? ‚Üí Send Telegram Error ‚Üí Stop ‚Üì Code: Clean Transcript (format with timestamps) ‚Üì HTTP Request: Get Metadata (RapidAPI) ‚Üì IF: Error? ‚Üí Send Telegram Error ‚Üí Stop ‚Üì Code: Convert Duration to Seconds ‚Üì Send Telegram: \u0026#34;Working on it...\u0026#34; ‚Üì AI Agent (Claude 4.5 Sonnet + Tavily Tool) ‚Üì Create Notion Page ‚Üì Log to Google Sheets ‚Üì Send Telegram: Markdown link to Notion page The entire process takes 30‚Äì60 seconds from link to finished notes.\nWhat I Learned # Start simple, but expect to iterate. My first version was \u0026ldquo;send URL, get transcript, ask AI to summarize.\u0026rdquo; The final version is far more sophisticated because I kept finding gaps.\nAPIs \u0026gt; scraping, always. The hours I wasted trying to scrape web pages could have been spent building features. APIs are worth the small cost.\nPrompt engineering is 80% of the work. The difference between mediocre and excellent AI output comes down to how well you structure your instructions.\nAI tools need guidance. You can\u0026rsquo;t just throw a tool at an AI and expect magic. You need to understand how the tool works and design your prompts accordingly.\nError handling is what makes it real. A workflow that fails gracefully and tells you why is infinitely more valuable than one that just breaks.\nThe best way to learn is to build something you\u0026rsquo;ll actually use. I built this because I genuinely wanted it. That kept me going through all the failed attempts.\nThe Result # Now, whenever I watch a YouTube video worth remembering, I paste the link into Telegram. A minute later, I will receive a message on Telegram with a link.\nSceenshot of the Telegram thread And the link will point to a Notion page with:\nA thumbnail and title Comprehensive notes with the creator\u0026rsquo;s voice preserved Clickable timestamps to jump to specific moments Key takeaways and insights Curated resources for deeper learning Preview of the summary on Notion It\u0026rsquo;s transformed how I consume and retain information from YouTube. Instead of passively watching and forgetting, I\u0026rsquo;m actively building a knowledge base I can reference forever.\nIf you\u0026rsquo;re thinking about building something similar, my advice: start messy, iterate relentlessly, and don\u0026rsquo;t be afraid to scrap approaches that aren\u0026rsquo;t working. The best automations are built through trial and error.\nAppendix: # The Final Prompt # Here\u0026rsquo;s the complete prompt I use in the AI Agent node (cleaned up for readability):\nYou are an expert note-taker who creates comprehensive, detailed notes from video transcripts. Your goal is to capture the substance, nuance, and creator\u0026#39;s voice. --- VIDEO INFORMATION: Title: {{ $json.title }} Channel: {{ $json.author }} Duration: {{ $json.lengthSeconds }} seconds Video URL: {{ $json.videoUrl }} FULL TRANSCRIPT WITH TIMESTAMPS: {{ $json.transcript }} --- CREATE DETAILED NOTES FOLLOWING THESE GUIDELINES: **CRITICAL TIMESTAMP RULES:** - ONLY use timestamps that appear in the transcript above - NEVER make up, estimate, or fabricate timestamps - Format: [MM:SS]({{ $json.videoUrl }}\u0026amp;t=XXXs) - Convert MM:SS to seconds (e.g., 2:30 = 150 seconds) START YOUR NOTES WITH: ![Thumbnail]({{ $json.thumbnail }}) **\u0026#34;{{ $json.title }}\u0026#34;** by {{ $json.author }} --- 1. Detect Content Type (Tutorial, Podcast, Educational, Review) 2. Preserve the creator\u0026#39;s voice with direct quotes 3. Use clickable timestamps frequently ### Overview Brief 2-3 sentence intro ### Main Content Detailed notes organized by topic, with timestamps ### Key Takeaways 5-10 actionable insights with timestamps ### Curated Resources for Further Learning Using web search, find 5-7 high-quality resources related to topics covered. --- RULES: - Be thorough (1,500-2,500 words) - Only use real timestamps from the transcript - Include specific examples, numbers, and details - Write naturally, as if YOU watched and took notes That\u0026rsquo;s it. If you build something similar, I\u0026rsquo;d love to hear about it. Happy automating!\n","date":"14 November 2025","externalUrl":null,"permalink":"/posts/youtube-summarizer/","section":"Blog","summary":"","title":"How I Built an Automated YouTube Note-Taker","type":"posts"},{"content":"Hi, I\u0026rsquo;m Safwan üëã\nI\u0026rsquo;m a [your profession/student/role] based in [location]. I\u0026rsquo;m passionate about [your interests].\nWhat I am working on What I wrote What I do # Brief description of what you do - work, studies, projects, hobbies, etc.\nRecent Projects # You can link to projects or write about them here. lolol\nCode Choreography Built an automation that extracts key insights from YouTube videos and sends them to email. Saves hours of manual note-taking every week. Built an automation that extracts key insights from YouTube videos and sends them to email. Saves hours of manual note-taking every week.\nServer-side is my jam(stack) Yuilt an automation that extracts key insights from YouTube videos and sends them to email. Saves hours of manual note-taking every week. Built an automation that extracts key insights from YouTube videos and sends them to email. Saves hours of manual note-taking every week.\n","date":"14 November 2025","externalUrl":null,"permalink":"/","section":"Safwan Siddiq","summary":"","title":"Safwan Siddiq","type":"page"},{"content":"Write your content here. Lets gooo\n","date":"22 October 2025","externalUrl":null,"permalink":"/posts/change-this-to-title/","section":"Blog","summary":"","title":"Change This to Title","type":"posts"},{"content":"","date":"17 October 2025","externalUrl":null,"permalink":"/posts/3rd-post/","section":"Blog","summary":"","title":"3rd Post","type":"posts"},{"content":"Write your content here.\n","date":"17 October 2025","externalUrl":null,"permalink":"/posts/4th-post/","section":"Blog","summary":"","title":"4th Post","type":"posts"},{"content":" One-Click Podcast Summaries to Inbox # I used to listen to long podcasts on YouTube and think:‚ÄúWow‚Ä¶ I should really take notes on this.‚Äù\nWhile doing the dishes. Or walking. Or in the car. Spoiler: I never took the notes. So I built a tool for myself. It‚Äôs just a form:\n‚Üí I drop in the YouTube link\n‚Üí Add my email\n‚Üí Hit submit Behind the scenes, it:\n‚Äì Scrapes the transcript (via Apify)\n‚Äì Summarizes the main points, stories, and lessons (OpenAI)\n‚Äì Sends me an email\n‚Äì And also saves it to Notion so I can refer back later I use it way more than I expected. It‚Äôs not fancy. It doesn‚Äôt have a name.\nBut it helps me learn from content I would‚Äôve otherwise let wash over me. And now when I hear something good in a 2-hour podcast, I don‚Äôt just think ‚ÄúI should remember this someday.‚Äù\nI just send the link‚Äîand move on.\nNow instead of forgetting, I just get a neat little summary in my inbox. And for once, it‚Äôs an unread email I actually want to open.\nHow to build it # Lorem ipsum Lorem ipsumLorem ipsumLorem ipsumLorem ipsum\nThis is H2 line # Lorem ipsumLorem ipsumLorem ipsumLorem ipsum\nanother h3 line # Lorem ipsumLorem ipsumLorem ipsumLorem ipsumLorem ipsumLorem ipsum Lorem ipsumLorem ipsumLorem ipsum Lorem ipsumLorem ipsumLorem ipsum Lorem ipsumLorem ipsumLorem ipsum\nis this a h4? # ","date":"17 October 2025","externalUrl":null,"permalink":"/posts/my-second-post/","section":"Blog","summary":"","title":"My Second Post","type":"posts"},{"content":" Overview # What the project is about\nMy Role # What you did\nTechnologies Used # Tech 1 Tech 2 Results/Outcome # What was achieved\n","date":"16 October 2025","externalUrl":null,"permalink":"/projects/project-name/","section":"Projects","summary":"","title":"Project Name","type":"projects"},{"content":"Here are some of my recent projects.\n","date":"16 October 2025","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":"","date":"16 October 2025","externalUrl":null,"permalink":"/tags/tag1/","section":"Tags","summary":"","title":"Tag1","type":"tags"},{"content":"","date":"16 October 2025","externalUrl":null,"permalink":"/tags/tag2/","section":"Tags","summary":"","title":"Tag2","type":"tags"},{"content":"","date":"16 October 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Welcome to my website! This is my first post using Hugo and Blowfish.\nWohoo\n","date":"3 October 2025","externalUrl":null,"permalink":"/posts/first-post/","section":"Blog","summary":"","title":"My First Post","type":"posts"},{"content":"This is the about page. I\u0026rsquo;ll add some info soon! Adding some changes\u0026hellip;.\n","externalUrl":null,"permalink":"/about/","section":"Safwan Siddiq","summary":"","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]